<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Zhe Huang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Zhe Huang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="cv.html">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zhe Huang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="pics/zhehuang.jpg" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><br />
<b>Zhe Huang</b><br /><br />
<a href="https://thehcalab.web.illinois.edu/">Human-Centered Autonomy Lab</a><br />
<a href="https://ece.illinois.edu/">Department of Electrical and Computer Engineering</a>, <a href="https://csl.illinois.edu/">Coordinated Science Laboratory</a><br />
<a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a><br /><br />
Email: zheh4@illinois.edu <br /><br />
[<a href="https://scholar.google.com/citations?user=ME-U3iwAAAAJ&amp;hl=en&amp;authuser=1">Google Scholar</a> |
<a href="https://github.com/tedhuang96">GitHub</a> | <a href="https://www.linkedin.com/in/zhehuang96/">LinkedIn</a>] <br /><br /></p>
</td></tr></table>
<h2>About me</h2>
<p>I'm a 6th-year Ph.D. candidate advised by Prof. Katherine Driggs-Campbell at University of Illinois Urbana-Champaign. I am also a research scientist at Meta.
I will receive the Ph.D. degree in Electrical and Computer Engineering from University of Illinois Urbana-Champaign in December, 2024.
I received the M.S. degree in Mechanical Engineering from Stanford University in 2019 and 
the B.Eng. degree in Energy and Power Engineering from Xi'an Jiaotong University in 2017.</p>
<p>My research is focused on building <b>Human-Centered Embodied AI</b> to enable robots to safely and efficiently interact with humans and the physical world.
A major challenge to achieve this goal is that existing fully autonomous robots do not have sufficient understanding of human behavior,
and act conservatively with humans around to guarantee safety of humans and themselves, which is at the cost of efficiency.
I develop human-centered autonomy frameworks including human prediction and robot planning with human intent and human trajectory
as interface for robots to achieve challenging human-involved open-world tasks.
My works integrate well-established algorithmic primitives and novel machine learning techniques to offer efficiency improvement under safety guarantees.
My works illustrate generality of Human-Centered Embodied AI across various applications including autonomous driving, crowd navigation,
collaborative manufacturing, and collaborative cooking.</p>
<p>My research areas are Robotics, Artificial Intelligence, and Human-Robot Interaction.</p>
<p>My Ph.D. thesis: <b>Bridging Prediction and Planning for Human-Centered Autonomy</b>.</p>
<h2>News</h2>
<ul>
<li><p>[2024.10] I passed my Ph.D. thesis defense.</p>
</li>
<li><p>[2024.08] Our interaction-aware conformal prediction paper <a href="https://www.algorithmic-robotics.org/papers/60_Interaction_aware_Conformal.pdf">ICP</a> is accepted by WAFR 2024.</p>
</li>
<li><p>[2024.07] I am excited to join Meta as a research scientist.</p>
</li>
<li><p>[2024.06] Our paper on <a href="https://arxiv.org/abs/2407.16771">Topology-Guided ORCA</a> is accepted by <a href="https://unsolvedsocialnav.org/">Unsolved Problems in Social Robot Navigation workshop</a> in conjunction with RSS 2024.</p>
</li>
<li><p>[2024.06] Our Embodied AI paper <a href="https://arxiv.org/abs/2406.13787">LIT</a> is selected for <b>Spotlight Presentation</b> by <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">the 3rd Workshop on Computer Vision in the Wild</a> at CVPR 2024.</p>
</li>
<li><p>[2024.05] Our Embodied AI paper <a href="https://arxiv.org/abs/2406.13787">LIT</a> is accepted by both <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">the 3rd Workshop on Computer Vision in the Wild</a> and <a href="https://embodied-ai.org/">the 5th Annual Embodied AI Workshop</a> at CVPR 2024.</p>
</li>
<li><p>[2024.04] We have released our main implementation for <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> on the GitHub repo <a href="https://github.com/tedhuang96/nirrt_star">nirrt_star</a>.</p>
</li>
<li><p>[2024.02] We have released our ROS implementation for <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> on the GitHub repo <a href="https://github.com/tedhuang96/PNGNav">PNGNav</a>.</p>
</li>
<li><p>[2024.01] Our path planning paper <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> is accepted by ICRA 2024.</p>
</li>
<li><p>[2023.01] Two papers <a href="https://arxiv.org/abs/2203.09063">Hierarchical Intetion Tracking</a> and <a href="https://sites.google.com/view/intention-aware-crowdnav/home">Intention Aware CrowdNav</a> are accepted by ICRA 2023.</p>
</li>
<li><p>[2022.12] I finished my internship at Amazon Robotics as an Advanced Robotics Research Co-op.</p>
</li>
<li><p>[2022.12] I am excited to attend <a href="http://www.robot-learning.ml/2022/">the 5th Robot Learning Workshop: Trustworthy Robotics</a> at NeurIPS 2022 as an invited speaker!</p>
</li>
<li><p>[2022.08] I am excited to join Amazon Robotics as an Advanced Robotics Research Co-op.</p>
</li>
<li><p>[2022.08] I finished my internship at Nuro as a PhD Intern.</p>
</li>
<li><p>[2022.08] We will present our work <a href="https://arxiv.org/abs/2206.01775">Seamless Interaction Design with Coexistence and Cooperation Modes for Robust Human-Robot Collaboration</a> on CASE 2022 Special Session on Adaptive and Resilient Cyber-Physical Manufacturing Networks.</p>
</li>
<li><p>[2022.05] I am excited to join Nuro as a PhD Intern.</p>
</li>
<li><p>[2022.05] Our work <a href="https://arxiv.org/abs/2205.14340">Insights from an Industrial Collaborative Assembly Project: Lessons in Research and Collaboration</a> is selected for <b>Spotlight Presentation</b> at <a href="https://sites.google.com/view/icra22ws-cor-wotf/accepted-papers?authuser=0">ICRA 2022 Workshop on Collaborative Robots and the Work of the Future</a>.</p>
</li>
<li><p>[2022.04] I am honored to give a talk at Wuhan University titled &ldquo;Human Behavior Modeling in Autonomous Driving and Collaborative Manufacturing&rdquo; (自动驾驶与协同制造中的人类行为建模).</p>
</li>
<li><p>[2022.02] Our demo &ldquo;Human-Robot Collaboration in Industrial Assembly Tasks&rdquo; is awarded the Best Robotics Demo in <a href="https://studentconference.csl.illinois.edu/">Coordinated Science Laboratory Student Conference</a>.</p>
</li>
<li><p>[2022.01] We will present <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> on ICRA 2022.</p>
</li>
<li><p>[2021.12] We have released our code for <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> on the GitHub repo <a href="https://github.com/tedhuang96/gst">gst</a>.</p>
</li>
<li><p>[2021.12] One paper <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> is accepted by RA-L.</p>
</li>
<li><p>[2021.06] We have released our code and pretrained models for <a href="https://sites.google.com/view/mif-wlstm">MIF-WLSTM</a> on the GitHub repo <a href="https://github.com/tedhuang96/mifwlstm">mifwlstm</a>.</p>
</li>
<li><p>[2020.11] One paper <a href="https://sites.google.com/view/mif-wlstm">MIF-WLSTM</a> is accepted by RA-L.</p>
</li>
<li><p>[2020.05] One paper is accepted by ITSC 2020.</p>
</li>
<li><p>[2020.01] One paper is accepted by RA-L and ICRA 2020.</p>
</li>
</ul>
<h2>Publications</h2>
<table class="imgtable"><tr><td>
<img src="pics/icp.png" alt="alt text" width="200px" height="111px" />&nbsp;</td>
<td align="left"><p><a href="https://www.algorithmic-robotics.org/papers/60_Interaction_aware_Conformal.pdf"><b>Interaction-aware Conformal Prediction for Crowd Navigation</b></a><br /><br />
<b>Zhe Huang</b>, Tianchen Ji, Heling Zhang, Fatemeh Cheraghi Pouria, Katherine Driggs-Campbell, Roy Dong <br /><br />
WAFR 2024 <br /><br />
[<a href="https://www.algorithmic-robotics.org/papers/60_Interaction_aware_Conformal.pdf">paper</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/lit.png" alt="alt text" width="200px" height="82px" />&nbsp;</td>
<td align="left"><p><a href="https://arxiv.org/abs/2406.13787"><b>LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration &ndash; A Robot Sous-Chef Application</b></a><br /><br />
<b>Zhe Huang</b>, John Pohovey, Ananya Yammanuru, Katherine Driggs-Campbell <br /><br />
<b>Spotlight Presentation</b> at CVPR 2024 <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Workshop on Computer Vision in the Wild</a> <br /><br />
CVPR 2024 Annual Embodied AI Workshop <br /><br />
[<a href="https://arxiv.org/abs/2406.13787">arXiv</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/nirrt*.png" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/nirrt-star"><b>Neural Informed RRT*: Learning-based Path Planning with Point Cloud State Representations under Admissible Ellipsoidal Constraints</b></a><br /><br />
<b>Zhe Huang</b>, Hongyu Chen, John Pohovey, Katherine Driggs-Campbell <br /><br />
ICRA 2024 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/10611099">paper</a>]
[<a href="https://arxiv.org/abs/2309.14595">arXiv</a>]
[<a href="https://sites.google.com/view/nirrt-star">project</a>]
[<a href="https://github.com/tedhuang96/nirrt_star">main code</a>]
[<a href="https://github.com/tedhuang96/PNGNav">ROS code</a>]
[<a href="https://youtu.be/xys6XxMqFqQ">presentation</a>]
[<a href="https://youtu.be/XjZqUJ0ufGA">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/hit.png" alt="alt text" width="200px" height="151px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/hierarchicalintentiontracking"><b>Hierarchical Intention Tracking for Robust Human-Robot Collaboration in Industrial Assembly Tasks</b></a> <br /><br />
<b>Zhe Huang</b>*, Ye-Ji Mun*, Xiang Li†, Yiqing Xie†, Ninghan Zhong†, Weihang Liang, Junyi Geng, Tan Chen, Katherine Driggs-Campbell <br /><br />
ICRA 2023 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/10160515">paper</a>]
[<a href="https://arxiv.org/abs/2203.09063">arXiv</a>]
[<a href="https://sites.google.com/view/hierarchicalintentiontracking">project</a>]
[<a href="https://youtu.be/lcSl-Jz3_mE">presentation</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/crowdnav++.gif" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/intention-aware-crowdnav/"><b>Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph</b></a> <br /><br />
Shuijing Liu, Peixin Chang, <b>Zhe Huang</b>, Neeloy Chakraborty, Kaiwen Hong, Weihang Liang, D. Livingston McPherson, Junyi Geng, Katherine Driggs-Campbell <br /><br />
ICRA 2023 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/10160660">paper</a>]
[<a href="https://arxiv.org/abs/2203.01821">arXiv</a>]
[<a href="https://sites.google.com/view/intention-aware-crowdnav/">project</a>]
[<a href="https://github.com/Shuijing725/CrowdNav_Prediction_AttnGraph">code</a>]
[<a href="https://youtu.be/boDDQvZ1yV0">presentation</a>]
[<a href="https://youtu.be/d9va6QW9sYA">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/gst.png" alt="alt text" width="200px" height="225px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9664278"><b>Learning Sparse Interaction Graphs of Partially Detected Pedestrians for Trajectory Prediction</b></a> <br /><br />
<b>Zhe Huang</b>, Ruohua Li, Kazuki Shin, Katherine Driggs-Campbell <br /><br />
RA-L with ICRA 2022 presentation option <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9664278">paper</a>]
[<a href="https://arxiv.org/abs/2107.07056">arXiv</a>]
[<a href="https://sites.google.com/view/gumbel-social-transformer">project</a>]
[<a href="https://github.com/tedhuang96/gst">code</a>]
[<a href="https://youtu.be/fHYg1zaMcxE">presentation</a>]
[<a href="https://youtu.be/oL2UlN53wUc">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/mifwlstm.gif" alt="alt text" width="200px" height="117px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9309334"><b>Long-Term Pedestrian Trajectory Prediction Using Mutable Intention Filter and Warp LSTM</b></a> <br /><br />
<b>Zhe Huang</b>, Aamir Hasan, Kazuki Shin, Ruohua Li, Katherine Driggs-Campbell <br /><br />
RA-L 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9309334">paper</a>]
[<a href="https://arxiv.org/abs/2007.00113">arXiv</a>]
[<a href="https://github.com/tedhuang96/mifwlstm">code</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/online_monitoring.png" alt="alt text" width="200px" height="153px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9294366"><b>Online Monitoring for Safe Pedestrian-Vehicle Interactions</b></a> <br /><br />
Peter Du, <b>Zhe Huang</b>†, Tianqi Liu†, Tianchen Ji†, Ke Xu†, Qichao Gao†,Hussein Sibai, Katherine Driggs-Campbell, Sayan Mitra<br /><br />
ITSC 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9294366">paper</a>]
[<a href="https://arxiv.org/abs/1910.05599">arXiv</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/softrobot_antenna.png" alt="alt text" width="200px" height="208px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/8972415"><b>3D Electromagnetic Reconfiguration Enabled by Soft Continuum Robots</b></a> <br /><br />
Lucia T. Gan, Laura H. Blumenschein, <b>Zhe Huang</b>, Allison M. Okamura, Elliot W. Hawkes, Jonathan A. Fan<br /><br />
RA-L 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/8972415">paper</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/drillbot.png" alt="alt text" width="200px" height="152px" />&nbsp;</td>
<td align="left"><p><a href="https://onepetro.org/SPEDC/proceedings-abstract/20DC/2-20DC/447222"><b>A Voice Interface for Drilling Systems</b></a> <br /><br />
Crispin Chatar, <b>Zhe Huang</b>, Peter Hadrovic<br /><br />
IADC/SPE International Drilling Conference and Exhibition 2020 <br /><br />
[<a href="https://onepetro.org/SPEDC/proceedings-abstract/20DC/2-20DC/447222">paper</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/fly-by-feel.png" alt="alt text" width="200px" height="153px" />&nbsp;</td>
<td align="left"><p><a href="https://www.dpi-proceedings.com/index.php/shm2019/article/view/32298"><b>High Accuracy Flight State Identification of a Self-Sensing Wing via Machine Learning Approaches</b></a> <br /><br />
<b>Zhe Huang</b>, Hongyi Zhao, Cheng Liu, Xi Chen, Fotis Kopsaftopoulos, Fu-Kuo Chang<br /><br />
Structural Health Monitoring 2019 <br /><br />
[<a href="https://www.dpi-proceedings.com/index.php/shm2019/article/view/32298">paper</a>]</p>
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
