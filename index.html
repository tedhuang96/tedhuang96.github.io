<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Zhe Huang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Zhe Huang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="cv.html">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zhe Huang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="pics/zhehuang.jpg" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><br />
<b>Zhe Huang</b><br /><br />
<a href="https://thehcalab.web.illinois.edu/">Human-Centered Autonomy Lab</a><br />
<a href="https://ece.illinois.edu/">Department of Electrical and Computer Engineering</a>, <a href="https://csl.illinois.edu/">Coordinated Science Laboratory</a><br />
<a href="https://illinois.edu/">University of Illinois at Urbana-Champaign</a><br /><br />
Email: zheh4@illinois.edu <br /><br />
[<a href="https://scholar.google.com/citations?user=ME-U3iwAAAAJ&amp;hl=en&amp;authuser=1">Google Scholar</a> |
<a href="https://github.com/tedhuang96">GitHub</a> | <a href="https://www.linkedin.com/in/zhehuang96/">LinkedIn</a>] <br /><br /></p>
</td></tr></table>
<h2>About me</h2>
<p>I'm a 5th-year Ph.D. candidate advised by Prof. Katherine Driggs-Campbell at University of Illinois at Urbana-Champaign. 
My research interest is in Human-Robot Interaction and Artificial Intelligence.
I do research on Learning-based Path Planning, Intention Tracking, Trajectory Prediction, Large Language Model Driven Robotics, and Integration of Prediction and Planning.
Applications of my research include Collaborative Manufacturing, Autonomous Driving, and Social Navigation.</p>
<p>I received the B.Eng. degree in Energy and Power Engineering from Xi'an Jiaotong University in 2017, 
and received the M.S. degree in Mechanical Engineering from Stanford University in 2019.</p>
<h2>News</h2>
<ul>
<li><p>[2024.04] We have released our main implementation for <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> on the GitHub repo <a href="https://github.com/tedhuang96/nirrt_star">nirrt_star</a>.</p>
</li>
<li><p>[2024.02] We have released our ROS implementation for <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> on the GitHub repo <a href="https://github.com/tedhuang96/PNGNav">PNGNav</a>.</p>
</li>
<li><p>[2024.01] Our path planning paper <a href="https://arxiv.org/abs/2309.14595">NIRRT*</a> is accepted by ICRA 2024.</p>
</li>
<li><p>[2023.01] Two papers <a href="https://arxiv.org/abs/2203.09063">Hierarchical Intetion Tracking</a> and <a href="https://sites.google.com/view/intention-aware-crowdnav/home">Intention Aware CrowdNav</a> are accepted by ICRA 2023.</p>
</li>
<li><p>[2022.12] I finished my internship at Amazon Robotics as an Advanced Robotics Research Co-op!</p>
</li>
<li><p>[2022.12] I am excited to attend <a href="http://www.robot-learning.ml/2022/">the 5th Robot Learning Workshop: Trustworthy Robotics</a> at NeurIPS 2022 as an invited speaker!</p>
</li>
<li><p>[2022.08] I am excited to join Amazon Robotics as an Advanced Robotics Research Co-op to work on grasp learning research projects.</p>
</li>
<li><p>[2022.08] I finished my internship at Nuro as a PhD Intern!</p>
</li>
<li><p>[2022.08] We will present our work <a href="https://arxiv.org/abs/2206.01775">Seamless Interaction Design with Coexistence and Cooperation Modes for Robust Human-Robot Collaboration</a> on CASE 2022 Special Session on Adaptive and Resilient Cyber-Physical Manufacturing Networks.</p>
</li>
<li><p>[2022.05] I am excited to join Nuro as a PhD Intern to work on autonomous driving research projects.</p>
</li>
<li><p>[2022.05] Our work <a href="https://arxiv.org/abs/2205.14340">Insights from an Industrial Collaborative Assembly Project: Lessons in Research and Collaboration</a> is selected for spotlight presentation at <a href="https://sites.google.com/view/icra22ws-cor-wotf/accepted-papers?authuser=0">ICRA 2022 Workshop on Collaborative Robots and the Work of the Future</a>.</p>
</li>
<li><p>[2022.04] I am honored to give a talk at Wuhan University titled &ldquo;Human Behavior Modeling in Autonomous Driving and Collaborative Manufacturing&rdquo; (自动驾驶与协同制造中的人类行为建模).</p>
</li>
<li><p>[2022.02] Our demo &ldquo;Human-Robot Collaboration in Industrial Assembly Tasks&rdquo; is awarded the Best Robotics Demo in <a href="https://studentconference.csl.illinois.edu/">Coordinated Science Laboratory Student Conference</a>.</p>
</li>
<li><p>[2022.01] We will present <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> on ICRA 2022.</p>
</li>
<li><p>[2021.12] We have released our code for <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> on the GitHub repo <a href="https://github.com/tedhuang96/gst">gst</a>.</p>
</li>
<li><p>[2021.12] One paper <a href="https://sites.google.com/view/gumbel-social-transformer">GST</a> is accepted by RA-L.</p>
</li>
<li><p>[2021.06] We have released our code and pretrained models for <a href="https://sites.google.com/view/mif-wlstm">MIF-WLSTM</a> on the GitHub repo <a href="https://github.com/tedhuang96/mifwlstm">mifwlstm</a>.</p>
</li>
<li><p>[2020.11] One paper <a href="https://sites.google.com/view/mif-wlstm">MIF-WLSTM</a> is accepted by RA-L.</p>
</li>
<li><p>[2020.05] One paper is accepted by ITSC 2020.</p>
</li>
<li><p>[2020.01] One paper is accepted by RA-L and ICRA 2020.</p>
</li>
</ul>
<h2>Publications</h2>
<table class="imgtable"><tr><td>
<img src="pics/nirrt*.png" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/nirrt-star"><b>Neural Informed RRT*: Learning-based Path Planning with Point Cloud State Representations under Admissible Ellipsoidal Constraints</b></a><br /><br />
<b>Zhe Huang</b>, Hongyu Chen, John Pohovey, and Katherine Driggs-Campbell <br /><br />
ICRA 2024 <br /><br />
[<a href="https://arxiv.org/abs/2309.14595">arXiv</a>]
[<a href="https://sites.google.com/view/nirrt-star">project</a>]
[<a href="https://github.com/tedhuang96/nirrt_star">main code</a>]
[<a href="https://github.com/tedhuang96/PNGNav">ROS code</a>]
[<a href="https://youtu.be/xys6XxMqFqQ">presentation</a>]
[<a href="https://youtu.be/XjZqUJ0ufGA">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/hit.png" alt="alt text" width="200px" height="151px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/hierarchicalintentiontracking"><b>Hierarchical Intention Tracking for Robust Human-Robot Collaboration in Industrial Assembly Tasks</b></a> <br /><br />
<b>Zhe Huang</b>*, Ye-Ji Mun*, Xiang Li†, Yiqing Xie†, Ninghan Zhong†, Weihang Liang, Junyi Geng, Tan Chen, and Katherine Driggs-Campbell <br /><br />
ICRA 2023 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/10160515">paper</a>]
[<a href="https://arxiv.org/abs/2203.09063">arXiv</a>]
[<a href="https://sites.google.com/view/hierarchicalintentiontracking">project</a>]
[<a href="https://youtu.be/lcSl-Jz3_mE">presentation</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/crowdnav++.gif" alt="alt text" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p><a href="https://sites.google.com/view/intention-aware-crowdnav/"><b>Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph</b></a> <br /><br />
Shuijing Liu, Peixin Chang, <b>Zhe Huang</b>, Neeloy Chakraborty, Kaiwen Hong, Weihang Liang, D. Livingston McPherson, Junyi Geng, and Katherine Driggs-Campbell <br /><br />
ICRA 2023 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/10160660">paper</a>]
[<a href="https://arxiv.org/abs/2203.01821">arXiv</a>]
[<a href="https://sites.google.com/view/intention-aware-crowdnav/">project</a>]
[<a href="https://github.com/Shuijing725/CrowdNav_Prediction_AttnGraph">code</a>]
[<a href="https://youtu.be/boDDQvZ1yV0">presentation</a>]
[<a href="https://youtu.be/d9va6QW9sYA">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/gst.png" alt="alt text" width="200px" height="225px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9664278"><b>Learning Sparse Interaction Graphs of Partially Detected Pedestrians for Trajectory Prediction</b></a> <br /><br />
<b>Zhe Huang</b>, Ruohua Li, Kazuki Shin, Katherine Driggs-Campbell <br /><br />
RA-L with ICRA 2022 presentation option <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9664278">paper</a>]
[<a href="https://arxiv.org/abs/2107.07056">arXiv</a>]
[<a href="https://sites.google.com/view/gumbel-social-transformer">project</a>]
[<a href="https://github.com/tedhuang96/gst">code</a>]
[<a href="https://youtu.be/fHYg1zaMcxE">presentation</a>]
[<a href="https://youtu.be/oL2UlN53wUc">demo</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/mifwlstm.gif" alt="alt text" width="200px" height="117px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9309334"><b>Long-Term Pedestrian Trajectory Prediction Using Mutable Intention Filter and Warp LSTM</b></a> <br /><br />
<b>Zhe Huang</b>, Aamir Hasan, Kazuki Shin, Ruohua Li, Katherine Driggs-Campbell <br /><br />
RA-L 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9309334">paper</a>]
[<a href="https://arxiv.org/abs/2007.00113">arXiv</a>]
[<a href="https://github.com/tedhuang96/mifwlstm">code</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/online_monitoring.png" alt="alt text" width="200px" height="153px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/9294366"><b>Online Monitoring for Safe Pedestrian-Vehicle Interactions</b></a> <br /><br />
Peter Du, <b>Zhe Huang</b>†, Tianqi Liu†, Tianchen Ji†, Ke Xu†, Qichao Gao†,Hussein Sibai, Katherine Driggs-Campbell, and Sayan Mitra<br /><br />
ITSC 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9294366">paper</a>]
[<a href="https://arxiv.org/abs/1910.05599">arXiv</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/softrobot_antenna.png" alt="alt text" width="200px" height="208px" />&nbsp;</td>
<td align="left"><p><a href="https://ieeexplore.ieee.org/abstract/document/8972415"><b>3D Electromagnetic Reconfiguration Enabled by Soft Continuum Robots</b></a> <br /><br />
Lucia T. Gan, Laura H. Blumenschein, <b>Zhe Huang</b>, Allison M. Okamura, Elliot W. Hawkes, and Jonathan A. Fan<br /><br />
RA-L 2020 <br /><br />
[<a href="https://ieeexplore.ieee.org/abstract/document/8972415">paper</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/drillbot.png" alt="alt text" width="200px" height="152px" />&nbsp;</td>
<td align="left"><p><a href="https://onepetro.org/SPEDC/proceedings-abstract/20DC/2-20DC/447222"><b>A Voice Interface for Drilling Systems</b></a> <br /><br />
Crispin Chatar, <b>Zhe Huang</b>, Peter Hadrovic<br /><br />
IADC/SPE International Drilling Conference and Exhibition 2020 <br /><br />
[<a href="https://onepetro.org/SPEDC/proceedings-abstract/20DC/2-20DC/447222">paper</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pics/fly-by-feel.png" alt="alt text" width="200px" height="153px" />&nbsp;</td>
<td align="left"><p><a href="https://www.dpi-proceedings.com/index.php/shm2019/article/view/32298"><b>High Accuracy Flight State Identification of a Self-Sensing Wing via Machine Learning Approaches</b></a> <br /><br />
<b>Zhe Huang</b>, Hongyi Zhao, Cheng Liu, Xi Chen, Fotis Kopsaftopoulos, Fu-Kuo Chang<br /><br />
Structural Health Monitoring 2019 <br /><br />
[<a href="https://www.dpi-proceedings.com/index.php/shm2019/article/view/32298">paper</a>]</p>
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
